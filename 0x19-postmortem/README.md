# 0x19. Postmortem

Any software system will eventually fail, and that failure can come stem from a wide range of possible factors: bugs, traffic spikes, security issues, hardware failures, natural disasters, human errorâ€¦ Failing is normal and failing is actually a great opportunity to learn and improve. Any great Software Engineer must learn from his/her mistakes to make sure that they wonâ€™t happen again. Failing is fine, but failing twice because of the same issue is not.

A postmortem is a tool widely used in the tech industry. After any outage, the team(s) in charge of the system will write a summary that has 2 main goals:

- To provide the rest of the companyâ€™s employees easy access to information detailing the cause of the outage. Often outages can have a huge impact on a company, so managers and executives have to understand what happened and how it will impact their work.
- And to ensure that the root cause(s) of the outage has been discovered and that measures are taken to make sure it will be fixed.

![Postmortem Report](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bSUlrrO7E83Eb85sAIiZrw.png)



# ğŸš¨ **Postmortem Report: The Day the Memory Leak Struck** ğŸš¨

![Escalation](https://miro.medium.com/v2/resize:fit:1172/format:webp/1*ty95YKSBnrpgSZ-_abaOIA.png)
*When the server suffers from memory leak....*

## Issue Summary

- **Duration:** The Great Memory Meltdown lasted for **2 hours and 45 minutes** from **3:15 PM to 6:00 PM UTC** on **August 12, 2024**.
  
- **Impact:** During the outage, our e-commerce website transformed from a sleek shopping experience to a sluggish beast. Users were clicking furiously, only to be met with spinning wheels and failed transactions. Approximately **85% of users** faced the wrath of the slow-down monster, and **20%** couldnâ€™t complete their purchases. Our sales team mourned the temporary revenue loss, and our customer service team suddenly became best friends with the word "Sorry!"

- **Root Cause:** Turns out, our servers werenâ€™t trying to take a nap; they were just choking on a memory leak in the session management service. The leak caused our servers to hog memory like it was going out of style, leading to the unfortunate slow-down.

## Timeline

Hereâ€™s how our day of doom unfolded:

- **3:15 PM UTC:** ğŸ¯ Issue detected via a monitoring alert screaming â€œHigh memory usage!â€
- **3:17 PM UTC:** ğŸ˜± Incident escalated to the on-call DevOps engineer, who was dreaming of a peaceful afternoon.
- **3:20 PM UTC:** ğŸ•µï¸â€â™‚ï¸ The blame game began. Was it the database again? (Spoiler: It wasnâ€™t.)
- **3:35 PM UTC:** ğŸ’¡ Database cleared of all charges; investigation shifted to the load balancer.
- **4:00 PM UTC:** ğŸ” Ah-ha! Server metrics pointed to a memory leak. Our servers werenâ€™t bloated; they were just full of unwanted junk!
- **4:20 PM UTC:** ğŸ” The session management service, recently updated with fancy new features, was singled out as the main suspect.
- **4:45 PM UTC:** ğŸš¨ Incident escalated to the developers who built the session management service.
- **5:15 PM UTC:** ğŸ› ï¸ Developers confirmed the presence of the leak. Time to plug it!
- **5:45 PM UTC:** ğŸ”§ A patch was deployed to kick the memory leak out.
- **6:00 PM UTC:** ğŸŸ¢ All systems go! The website was back to its former glory.

## Root Cause and Resolution

- **Root Cause:** Our session management service was hoarding memory like a squirrel hoards nuts for winter. Session objects werenâ€™t being properly released from memory, causing the servers to eventually buckle under the weight. The culprit? A recent update that introduced a new session tracking feature but forgot to pack the garbage bags.

- **Resolution:** Our sharp-eyed developers jumped in, pinpointed the faulty code, and implemented a patch. This fix made sure session objects were correctly garbage-collected, so our servers could breathe easy again. After some rigorous testing in the staging environment, the patch was rolled out to production, and peace was restored.

## Corrective and Preventative Measures

Letâ€™s make sure this doesnâ€™t happen again:

- **Improvements:**
  1. **Code Review Process:** Weâ€™re beefing up our code reviews to catch memory management mishaps before they hit production.
  2. **Monitoring Enhancements:** Weâ€™re adding extra eyes on memory usage in the session management service to catch leaks before they become a deluge.
  3. **Load Testing:** Our load tests will now include long-duration stress tests, so we can spot memory issues lurking in the shadows.

- **Tasks:**
  1. **Patch Nginx server** to improve session handling and reduce memory overhead.
  2. **Add memory usage monitoring** to our APM tool, focused on the session management service.
  3. **Review and refactor the session management service codebase** to ensure it plays nicely with memory.
  4. **Conduct a post-incident review** with the dev team to brainstorm further improvements.
  5. **Document best practices** for memory management in our development guidelines.

![Memory Leak Diagram](https://resources.jetbrains.com/help/img/idea/2024.2/cpu-memory-charts-leak_dark.png)  
*Figure 1: How the memory leak took down our servers.*

